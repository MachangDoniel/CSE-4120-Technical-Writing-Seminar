\section*{}
\begin{center}
    {\fontsize{14}{1.5}\selectfont \textbf{CHAPTER IV}}\\
    \vspace{12pt}
    {\fontsize{16}{1.5}\selectfont \textbf{Implementation, Results and Discussions}}\\
    \vspace{12pt}
    \vspace{12pt}
\end{center}

\setcounter{section}{4}
\setcounter{subsection}{0}
\addcontentsline{toc}{section}{\textbf{CHAPTER IV Implementation, Results and Discussion}} % Add to ToC
\renewcommand{\theequation}{\thesection.\arabic{equation}}
\renewcommand{\thetable}{\thesection.\arabic{table}}
\renewcommand{\thefigure}{\thesection.\arabic{figure}}
\setcounter{table}{0}
\setcounter{figure}{0}

\setcounter{equation}{0}
\setlength{\parindent}{0pt}
\usetikzlibrary{positioning, shapes}
\tikzstyle {rect} = [rectangle, minimum width = 4cm, minimum height = 4cm, text width = 3cm, align = center, draw = black, fill = white!30]
\tikzstyle{box} = [rectangle, minimum width = 1cm, minimum height = 1cm, align = center, draw = black, fill = white]
\tikzstyle {arrow} = [line width = 1.2mm, ->, >=stealth, black]

\subsection{Experimental Setup}

\subsubsection{Software Environment}

The software environment used for the experiments included:

\begin{itemize}
    \item \textbf{Python 3.8}: A version of the Python programming language, offering new features like assignment expressions (the walrus operator :=), positional-only arguments, and improved performance.
    
    \item \textbf{TensorFlow 2.4}: An open-source machine learning framework by Google, designed for building and deploying machine learning models, offering high-level APIs like Keras for ease of use.
    
    \item \textbf{Keras}: A high-level neural networks API, written in Python and capable of running on top of TensorFlow, that allows for easy and fast prototyping of deep learning models.
    
    \item \textbf{Scikit-learn}: A machine learning library for Python that provides simple and efficient tools for data mining and data analysis, built on NumPy, SciPy, and matplotlib.
    
    \item \textbf{Pandas}: An open-source data manipulation and analysis library for Python, offering data structures like DataFrame and Series to handle and analyze structured data easily.
    
    \item \textbf{NumPy}: A fundamental package for scientific computing in Python, providing support for large multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays.
    
\end{itemize}

\subsubsection{Hardware Configuration}

The experiments were conducted on a machine with the following specifications:

\begin{itemize}

\item \textbf{CPU}: Intel Core i7,  Intel Xeon Gold, Intel Core i7

\item \textbf{GPU}: NVIDIA GTX 1080 Ti, NVIDIA Tesla V100

\item \textbf{RAM}: 32GB, 64GB, 16GB

\end{itemize}

\subsection{Dataset}
\textbf{Source of paper 1:} A telecommunications company dataset containing customer demographics, account information, and service usage.
Size: 10,000 samples with 15 features each.
Preprocessing: Missing values imputation, normalization of continuous features, and one-hot encoding of categorical features.

\textbf{Source of paper 2:}Source: A telecom companyâ€™s customer data including demographic details, account information, and usage statistics.
Size: 20,000 records with 20 features each.
Preprocessing: Data cleaning (removal of duplicates, handling missing values), normalization, and categorical variable encoding.

\textbf{Source of paper 3:} Telecommunication company data on customer behavior and service usage.
Size: 15,000 samples with 12 features each.
Preprocessing: Data cleaning (handling missing values, duplicates), normalization, and encoding of categorical features.

\begin{table}[htbp]
    \centering
    \caption{Table indicating the dataset size}
    \label{tab:dataset}
    \begin{tabular}{|l|c|c|}
    \hline
    \textbf{Dataset} & \textbf{Size} & \textbf{Features} \\ \hline
    Paper 1 & 10k & 15 \\ \hline
    Paper 2 & 20K & 20 \\ \hline
    Paper 3 & 15K & 12 \\ \hline
    \end{tabular}

\end{table}
\newpage
\subsection{Implementation and Results}

\subsubsection{Quantitative Results}

\begin{longtable}{| m{1.5cm} | m{3cm} | m{1.5cm} | m{1.5cm} | m{1.5cm} | m{2cm} |}
\caption{Performance Metrics for Various Models} 
\label{tab:performance_metrics} \\ 
\hline
\textbf{Paper} & \textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F-Measure} \\ 
\hline
\endfirsthead
\hline
\textbf{Paper} & \textbf{Model} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F-Measure} \\ 
\hline
\endhead
\multirow{Paper 1} & DL                 & 91.42 & 83.04 & 81.66 & 82.34 \\ \cline{2-6}
                         & NN                 & 93.94 & 89.96 & 84.42 & 87.10 \\ \cline{2-6}
                         & AutoMLP            & 93.91 & 89.45 & 84.92 & 87.12 \\ \cline{2-6}
                         & Bagging DL         & 91.51 & 90.67 & 72.94 & 80.84 \\ \cline{2-6}
                         & AdaBoost DL        & 91.09 & 82.16 & 81.55 & 81.85 \\ \cline{2-6}
                         & Bagging NN         & 94.00 & 92.48 & 86.20 & 89.23 \\ \cline{2-6}
                         & AdaBoost NN        & 93.07 & 87.37 & 83.48 & 85.38 \\ \cline{2-6}
                         & Bagging MLP        & 94.15 & 92.23 & 82.99 & 87.37 \\ \cline{2-6}
                         & AdaBoost MLP       & 93.88 & 89.41 & 84.82 & 87.05 \\ \hline
\multirow{Paper 2} & ANN+ANN   & 94.32 & - & - & -     \\ \cline{2-6}
                         & SOM+ANN      & 93.06 & - & - & -     \\ \hline
\multirow{Paper 3} & NB(Balanced Data)                 & 91.95 & - & - & -     \\ \cline{2-6}
                        & NB(Unbalanced Data)                 & 84.75 & - & - & -     \\ \cline{2-6}
                        & SVM(Balanced Data)                 & 90.8 & - & - & -     \\ \cline{2-6}
                         & SVM(Unbalanced Data) & 81.05 & -     & -     & -     \\ \hline
\end{longtable}







\subsubsection{Qualitative Results}
\begin{enumerate}

\item \textbf{Deep Learning (DL):} Achieved high accuracy and balanced performance across precision, recall, and F-measure. It's a strong performer overall, indicating DL's effectiveness in the task.

\item \textbf{Neural Networks (NN):} Similar to DL but with slightly higher precision and recall. It's a reliable alternative to DL with comparable performance.

\item \textbf{AutoMLP:} Shows similar performance to NN, indicating that automated machine learning can achieve results comparable to hand-tuned models.

\item \textbf{Bagging and AdaBoost DL/NN/MLP:} These ensemble methods demonstrate mixed results. While some achieve high accuracy and precision, there's a noticeable drop in recall for Bagging DL, indicating potential class imbalance issues.

\item \textbf{Support Vector Machine (SVM):} Performance varies significantly based on class balancing. Balanced SVM outperforms unbalanced SVM in accuracy, precision, and recall, emphasizing the importance of handling class imbalance.

\item \textbf{Naive Bayes (NB):} Similar to SVM, balancing classes improves performance. NB generally performs well, particularly in balanced settings, showcasing its simplicity and effectiveness for classification tasks.

\item \textbf{Naive Bayes (NB):} Consistently high performance across accuracy, precision, and recall, indicating its robustness for the task.

\item \textbf{Logistic Regression:} While accuracy is lower compared to other models, precision, recall, and F-measure are not reported. More insights are needed to assess its performance comprehensively.

Overall, DL-based models, along with well-tuned traditional classifiers like SVM and NB, show promise for the classification task. However, class imbalance seems to affect some models, highlighting the importance of preprocessing techniques or alternative algorithms to address this issue. Additionally, further investigation is needed for models like logistic regression to understand their performance comprehensively.
\end{enumerate}